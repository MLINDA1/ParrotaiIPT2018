Based on deep learning i had covered the following concepts
  * The general introduction on deep learning
The Neural network in which we had the following
  * Convolution Neural Networks
  * Recurent Neural Networks
Now let's see a bit about the concepts of neural networks according to what i had understand.
The neural netwoks as far as deep learning is concern is an extremly simplfied model of the human brain
which transforms the input into out put to the best of it's ability,it composed of many neurons the co-oparate
to perform a desired function.
The neural netwoks is used for 
* Classification
* Noise Reduction
* Prediction
Now based on what i had understood there are some of factors that lead to the use of the neural netowks:
* The ability to lean
* The abilty to understand
But also i had covered on how the neural networks works 
# Activation function
Where we had the following
* Sigmoid activation function
* Tanh activatiomm function
* Relu activation function
# The concepts of backpropagation
The basic backpropagation algorith is based on minimizing the error ot the network using the derivative of the 
error function simple,slow and prone to local minima issues.
# Hidden layers and neurons
For most problems one layer is suffient but two layers are required when the function is discontious,also 
the number of layers are important.
-Too few lead to the underfiting the data
-Too many also lead to the overfiting
# The convolutional 
In this part i had covered the following thing
* Convolution layer
* Pooling layer
* Normalization layer
* fully connected layer
* And how to impliment this model using the pytorch
# The recurent Neural netwoks
where i had learnt the following concepts
* The general RNN concepts/an introductuion to RNN
* Iplimenting the RNN using pytorch and Theano
* The backpropagation and understanding the vanishing gradient and exploding the gradient problem in a model
* Impliment the GRU/LSTM RNN
